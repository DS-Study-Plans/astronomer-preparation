Apache airflow is an open source platform to programmatically author, schedule and
monitor workflows

* It's dynamical, in the sense that can run arbitrary python code
* Can scale to execute the task in a cluster using celery
* It's interactive with a UI, CLI or REST API
* Extensible with custom plugins

It's not suited for a streaming solution like spark

It uses three components:

* **Web server:** It's used to access the UI
* **Scheduler:** Helps to trigger a task with certain rules
* **Metadata Database:** Save the recollected data by Airflow

The executor defines how you are going to run your tasks, the worker is where the task
are executed.


## Process:

The workflow usually follows the current lifecycle:

* Create a file, e.g dag.py in folder dags where the pipeline is going to be defined
* The dag.py is parsed into the web server and schedule, each within 30 seconds and 5 minutes
for new files
* Scheduler creates a DagRun object in the datastore with no status
* A task instance is created with the status scheduler
* The task instance is sent to the executor with status queue
* The worker takes the task and start to run it, change the status to running
* Executor update the metastore
* If the task works, the status is updated to success



