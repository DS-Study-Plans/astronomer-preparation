Apache airflow is an open source platform to programmatically author, schedule and
monitor workflows

* It's dynamical, in the sense that can run arbitrary python code
* Can scale to execute the task in a cluster using celery
* It's interactive with a UI, CLI or REST API
* Extensible with custom plugins

It's not suited for a streaming solution like spark

IT uses three components:

* **Web server:** It's used to access the UI
* **Scheduler:** Helps to trigger a task with certain rules
* **Metadata Database:** Save the recollected data by Airflow

The executor defines how you are going to run your tasks, the worker is where the task
are executed.
